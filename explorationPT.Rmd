---
title: "Revisão de Literatura"
output:
  html_document: default
  word_document: default
---

```{r,echo=FALSE, message=FALSE, results='hide', warning=FALSE}
# Loading libraries
library(ggplot2)
library(splitstackshape)
library(igraph)
library(knitr)

# Set ggplot theme
theme_set(theme_minimal(12))
```


```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
# Loading and preparing data

# Call cleaning2.R to process the data in the input folder and
# save processed files to output folder
source("cleaning2.R", chdir = T)

# Load yearly publication data
years <- read.table("analyze.csv", sep = ";", header = T)

# Helper function to remove leading and trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

# Fixing variable types
# (Manual conversion to chararcters could be avoided by setting
# stringsAsFactors=FALSE in read.delim() function in cleaning2.R. Oh well.)
literature$AuthorFullName <- as.character(literature$AuthorFullName)
literatureByAuthor$AuthorFullName <- as.character(literatureByAuthor$AuthorFullName)
literatureByKeywords$AuthorFullName <- as.character(literatureByKeywords$AuthorFullName)
literatureByCategory$AuthorFullName <- as.character(literatureByCategory$AuthorFullName)

literature$Abstract <- as.character(literature$Abstract)
literature$DocumentTitle <- as.character(literature$DocumentTitle)

literature$YearPublished <- as.numeric(as.character(literature$YearPublished))

literature$CitedReferences <- as.character(literature$CitedReferences)

literature$TimesCited <- as.numeric(literature$TimesCited)
literatureByAuthor$TimesCited <- as.numeric(literatureByAuthor$TimesCited)
literatureByKeywords$TimesCited <- as.numeric(literatureByKeywords$TimesCited)
literatureByCategory$TimesCited <- as.numeric(literatureByCategory$TimesCited)

literature$AuthorKeywords <- as.character(literature$AuthorKeywords)

literatureByKeywords$AuthorKeywords <- as.character(literatureByKeywords$AuthorKeywords)
```
Este relatório fornece uma análise dos registros baixados do [Web of Science](http://webofscience.com). A análise identifica os importantes autores, periódicos e palavras-chave no conjunto de dados com base no número de ocorrências, na contagem de citações e referências. Uma rede de citações dos registros fornecidos é criada e usada para identificar os documentos importantes de acordo com seu grau, contagem total de citações e pontuações no PageRank. A análise também encontra referências citadas com frequência que não foram incluídas no conjunto de dados original baixado da Web of Science.

Os relatórios também podem ser gerados através do [Serviço de análise online](http://hammer.nailsproject.net/), e o código-fonte está disponível no [GitHub](https://github.com/aknutas/nails). As instruções e links para vídeos tutoriais podem ser encontrados na [página do projeto](https://aknutas.github.io/nails/). Para obter mais detalhes, consulte o seguinte artigo: [Knutas, A., Hajikhani, A., Salminen, J., Ikonen, J., Porras, J., 2015. Cloud-Based Bibliometric Analysis Service for Systematic Mapping Studies. CompSysTech 2015](http://www.codecamp.fi/lib/exe/fetch.php/wiki/nails-compsystech2015-preprint.pdf). O uso do código é gratuito e por esse motivo pedimos, por favor, cite nosso trabalho de pesquisa em bibliometria se você publicar os resultados da análise..

O conjunto de dados analisado consiste de `r nrow(literature)` registros com `r ncol(literature)` variáveis. Para mais informações sobre as variáveis você pode encontrar acessando o portal WoS [Web of Science](https://images.webofknowledge.com/WOK46/help/WOS/h_fullrec.html).

## Anos de Publicação
```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(literature, aes(YearPublished)) +
  geom_histogram(binwidth = 1, fill = "darkgreen") +
  ggtitle("Anos publicados") +
  xlab("Anos") +
  ylab("Contagem de Artigos")

# Calculo do volume de contagem relativa de publicação
yearTable <- table(literature$YearPublished)    # Tabulate publication years
yearDF <- as.data.frame(yearTable)              # Turn to dataframe
names(yearDF) <- c("Year", "Freq")              # Fix column names

# Merge to dataframe of total publication numbers (years)
yearDF <- merge(yearDF, years, by.x = "Year", by.y = "Year", 
                all.x = TRUE)
yearDF$Year <- as.numeric(as.character(yearDF$Year))    # factor to numeric
# Calculate published articles per total articles by year
yearDF$Fraction <- yearDF$Freq / yearDF$Records
```

## Volume relativo de publicação
```{r, echo=FALSE}
ggplot(yearDF, aes(Year, Fraction, group = 1)) +
  geom_line(color = "darkgreen") +
  xlab("Anos") +
  ylab("Fração de todas as publicações") +
  ggtitle("Volume relativo de publicação")
```


## Autores Importantes
Classificado pelo número de artigos publicados e pelo número total de citações.
```{r,echo=FALSE}
# Calculating total number of citations for each author
citationSums <- aggregate(literatureByAuthor$TimesCited,
    by = list(AuthorFullName = toupper(literatureByAuthor$AuthorFullName)),
    FUN = sum, na.rm = T)

# Fixing column names
names(citationSums) <- c("AuthorFullName", "TotalTimesCited")
# Crating new data frame to plot citations by author

# Extract author names
authorNames <- unlist(strsplit(literature$AuthorFullName, ";"))
# Remove apostrophes
authorNames <- gsub("'", "", authorNames)
# Count author name frequencies
authors <- table(authorNames)
# Transform to a data frame
authors <- as.data.frame(authors)
# Merge with data frame containing the total times citated by each author
authors <- merge(authors, citationSums, by.x = "authorNames",
              by.y = "AuthorFullName" )
# Fix column name
names(authors)[1] <- "AuthorFullName"
# Sort the table by total times sited, decreasing order
authors <- authors[with (authors, order(-TotalTimesCited)), ]

# Sort authors by number of articles, extract top 25,
# and reorder factors for plotting
authors <- authors[with (authors, order(-Freq)), ]
authorsPop <- head(authors, 25)
authorsPop <- transform(authorsPop, AuthorFullName = reorder(AuthorFullName, Freq))

ggplot(authorsPop, aes(AuthorFullName, Freq)) +
    geom_bar(stat = "identity", fill = "blue") +
    coord_flip() +
    ggtitle("Produção dos autores") +
    xlab("Autores") +
    ylab("Número dos artigos")
```

```{r,echo=FALSE}
# Reorder AuthorFullName factor according to TotalTimesCited (decreasing order)
authors <- transform(authors,
                            AuthorFullName = reorder(AuthorFullName,
                                                     TotalTimesCited))

ggplot(head(authors, 25), aes(AuthorFullName, TotalTimesCited)) +
    geom_bar(stat = "identity", fill = "blue") +
    coord_flip() +
    ggtitle("Autores mais citados") +
    xlab("Autor") + ylab("Total de citações")
```

## Periódicos Importantes
Classificado pelo número de artigos publicados no conjunto de dados e número total de citações.
```{r,echo=FALSE}
# Calculating total citations for each publication.
# Functionality same as for the authors, see above.

citationSums <- aggregate(literature$TimesCited,
    by = list(PublicationName= literature$PublicationName),
    FUN = sum, na.rm = T)
names(citationSums) <- c("PublicationName", "PublicationTotalCitations")
citationSums <- citationSums[with (citationSums, order(-PublicationTotalCitations)), ]
top25 <- head(citationSums[,c("PublicationName", "PublicationTotalCitations")], 25)
top25$PublicationName <- strtrim(top25$PublicationName, 50)

publications <- table(literature$PublicationName)
publications <- as.data.frame(publications)
names(publications) <- c("Publication", "Count")
publications <- publications[with (publications, order(-Count)), ]

publications$Publication <- strtrim(publications$Publication, 50)
publications <- transform(publications, Publication = reorder(Publication, Count))

literature <- merge(literature, citationSums,
                    by = "PublicationName" )

ggplot(head(publications, 25), aes(Publication, Count)) +
    geom_bar(stat = "identity", fill = "orange") +
    coord_flip() +
    theme(legend.position = "none") +
    ggtitle("Periódicos mais populares") +
    xlab("Revistas") +
    ylab("Contagem de artigos")
```


```{r,echo=FALSE}
top25 <- transform(top25,
                          PublicationName = reorder(PublicationName,
                                                    PublicationTotalCitations))
ggplot(top25,
       aes(PublicationName, PublicationTotalCitations)) +
    geom_bar(stat = "identity", fill = "orange") +
    coord_flip() +
    theme(legend.position = "none") +
    ggtitle("Periódicos mais citados") +
    xlab("Revistas") + ylab("Total de citações")
```


## Palavras chaves importantes
Classificado pelo número de artigos em que a palavra-chave é mencionada e pelo número total de citações para a palavra-chave.
```{r, echo=FALSE}
# Calculating total citations for each keyword
# Functionality same as for the authors, see above.

# Sometimes AuthorKeywords column is empty.
# Following if-else hack prevents crashing in those situations,
# either by using KeywordsPlus column or skipping keyword analysis.
if (using_KeywordsPlus == TRUE) {
  cat("No keywords. Using KeywordsPlus instead.")
  names(literature)[c(21, 22)] <- c("AuthorKeywordsTemp", "AuthorKeywords")
}

if (nrow(literatureByKeywords) == 0) {
  cat("No keywords")
} else {
  # Below functionality same as above for important authors.
  keywordCitationSum <- aggregate(literatureByKeywords$TimesCited,
                                by = list(AuthorKeywords =
                            literatureByKeywords$AuthorKeywords), FUN = sum,
                            na.rm = T)
  names(keywordCitationSum) <- c("AuthorKeywords", "TotalTimesCited")

  keywords <- unlist(strsplit(literature$AuthorKeywords, ";"))
  keywords <- trim(keywords)
  keywords <- table(keywords)
  keywords <- as.data.frame(keywords)
  names(keywords) <- c("AuthorKeywords", "Freq")

  keywords <- merge(keywords, keywordCitationSum, by = "AuthorKeywords")

  keywords <- keywords[with (keywords, order(-Freq)), ]
  keywordsPop <- head(keywords, 25)
  keywordsPop <- transform(keywordsPop, AuthorKeywords =
                             reorder(AuthorKeywords, Freq))

  ggplot(keywordsPop, aes(AuthorKeywords, Freq)) +
    geom_bar(stat = "identity", fill = "purple") +
    coord_flip() +
    ggtitle("Palavras Chaves populares") +
    xlab("Palavra Chave") +
    ylab("Número de ocorrência")
}
```

```{r,echo=FALSE}
if (nrow(literatureByKeywords) > 0) {
  keywords <- keywords[with (keywords, order(-TotalTimesCited)), ]
  keywords <- transform(keywords, AuthorKeywords =
                             reorder(AuthorKeywords, TotalTimesCited))
  ggplot(head(keywords, 25), aes(AuthorKeywords, TotalTimesCited)) +
    geom_bar(stat = "identity", fill = "purple") +
    coord_flip()  +
    ggtitle("Palavras Chaves mais citadas") +
    xlab("Palavra Chave") + ylab("Total de Citação")
}

# Change the column names back to original ones
names(literature)[c(21, 22)] <- c("AuthorKeywords", "KeywordsPlus")
```

## Artigos importantes
Os artigos mais importantes e outras fontes são identificados abaixo usando três medidas importantes: 1) grau na rede de citações, 2) contagem de citações fornecidas pelo Web of Science (apenas para artigos incluídos no conjunto de dados) e 3) pontuação do PageRank em rede de citações. Os 25 documentos com maior pontuação são identificados usando essas medidas separadamente. Os resultados são combinados e as duplicatas são removidas. Os resultados são classificados por grau e os vínculos são separados pela contagem de citações e depois pelo PageRank.

Quando um [Digital Object Identifier (DOI)](http://www.doi.org/) for disponível,  o documento completo pode ser encontrado usando [Resolve DOI](https://dx.doi.org/) website.

```{r,echo=FALSE}
# Create igraph
citationGraph <- graph.data.frame(citationEdges, vertices = citationNodes)
# Calculate PageRanks
citationNodes$PageRank <- page.rank(citationGraph)$vector
# Calculate in-degrees
citationNodes$InDegree <- degree(citationGraph, mode = "in")

# Extract the articles included in the data set and articles not included
# in the dataset
citationsLit <- citationNodes[citationNodes$Origin == "literature", ]
citationsRef <- citationNodes[citationNodes$Origin == "reference", ]
# Merge with selected columns in literature data frame
citationsLit <- merge(citationsLit,
                      literature[, c("ReferenceString",
                                     "DocumentTitle")],
                       by.x = "FullReference", by.y = "ReferenceString")
# Create article strings (document title, reference information and abstract
# separated by "|")
citationsLit$Article <- paste(toupper(citationsLit$DocumentTitle), " | ",
                              citationsLit$FullReference, " | ",
                                      citationsLit$Abstract)

# Trim FullReference to 100 characters
citationsLit$FullReference <- strtrim(citationsLit$FullReference, 100)
citationsRef$FullReference <- strtrim(citationsRef$FullReference, 100)
```

### Inclusos no banco de dados processados
Esses documentos foram incluídos nos `r nrow(literature)` registros baixados da Web of Science.
```{r, echo=FALSE}
# Sort citationsLit by TimesCited, decreasing
citationsLit <- citationsLit[with (citationsLit, order(-TimesCited)), ]
# Extract top 25
topLit <- head(citationsLit, 25)
# Sort by InDegree, decreasing
citationsLit <- citationsLit[with (citationsLit, order(-InDegree)), ]
# Add to list of top 25 most cited papers
topLit <- rbind(topLit, head(citationsLit, 25))
# Sort by PageRank, decreasing
citationsLit <- citationsLit[with (citationsLit, order(-PageRank)), ]
# Add to list of most cited and highest InDegree papers
topLit <- rbind(topLit, head(citationsLit, 25))
# Remove duplicates
topLit <- topLit[!duplicated(topLit[, "FullReference"]), ]
# Sort topLit by InDegree, break ties by TimesCited, then PageRank.
topLit <- topLit[with (topLit, order(-InDegree, -TimesCited, -PageRank)), ]
# Print list
kable(topLit[, c("Article", "InDegree", "TimesCited","PageRank")])
```

### Não inclusos no banco de dados processados
Esses artigos e outras referências não estavam entre os `r nrow(literature)` registros baixados da Web of Science.
```{r, echo=FALSE}
# Sort citationsRef by InDegree, decreasing
citationsRef <- citationsRef[with (citationsRef, order(-InDegree)), ]
# Extract top 25
topRef <- head(citationsRef, 25)
# Sort by PageRank, decreasing
citationsRef <- citationsRef[with (citationsRef, order(-PageRank)), ]
# Add to list of highes in degree papers (references)
topRef <- rbind(topRef, head(citationsRef, 25))
# Remove duplicates
topRef <- topRef[!duplicated(topRef[, "FullReference"]), ]
# Sort by InDegree, break ties by PageRank
topRef <- topRef[with (topRef, order(-InDegree, -PageRank)), ]
# Print results
kable(topRef[, c("FullReference", "InDegree", "PageRank")])
```

## Periódicos mais referenciados
```{r, echo=FALSE}
references <- unlist(strsplit(literature$CitedReferences, ";"))

get_publication <- function(x) {
    publication <- "Not found"
    try(
        publication <- unlist(strsplit(x, ","))[[3]],
        silent = TRUE
    )
    return(publication)
}

refPublications <- sapply(references, get_publication)
refPublications <- sapply(refPublications, trim)
refPublications <- refPublications[refPublications != "Not found"]
refPublications <- as.data.frame(table(refPublications))
names(refPublications) <- c("Publication", "Count")
refPublications <- refPublications[with (refPublications, order(-Count)), ]

refPublications <- transform(refPublications,
                             Publication = reorder(Publication, Count))

ggplot(head(refPublications, 25), aes(Publication, Count)) +
    geom_bar(stat = "identity", fill = "orange") +
    coord_flip() +
    theme(legend.position = "none") +
    ggtitle("Periódicos mais referenciados") +
    xlab("Revistas") +
    ylab("Quantitativo")
```

```{r, results='asis', echo=FALSE}
if (enableTM) {
cat('## Saída via Modelagem de Tópicos
[Modelagem de Tópicos](https://en.wikipedia.org/wiki/Topic_model) um tipo de método estatístico de mineração de texto para descobrir “tópicos” comuns que ocorrem em uma coleção de documentos. Um algoritmo de modelagem de tópicos examina essencialmente os resumos incluídos nos conjuntos de dados em busca de clusters de co-ocorrência de palavras e os agrupa por um processo de similaridade..

As colunas a seguir descrevem cada tópico detectado usando [LDA topic modeling](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) listando as dez palavras mais características em cada tópico. Consulte também a [Visualização interativa ](output/topicmodelvis/index.html) para uma melhor caracterização dos tópicos e uma representação visual de quão (des) semelhantes os tópicos detectados são entre si.

O número de tópicos são estimados utilizando as [bibliotecas estruturais de modelagem de tópicos](https://cran.r-project.org/web/packages/stm/index.html) por intermédio da coerência semântica da biblioteca de modelos de tópicos estruturais. Os valores brutos estão disponíveis no arquivo de saída - output como kqualityvalues.csv e podem ser interpretados com a [stm documentation](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) se necessário (leia a seção 3.4). A pesquisa é limitada entre quatro e seis tópicos por motivos de desempenho do servidor.
')
}
```

```{r, echo=FALSE}
if (enableTM) {
  tw <- data.frame(topickeywords)
  colnames(tw) <- gsub('X', 'Topic ', colnames(tw))
  kable(tw, col.names = colnames(tw))
}
```
